[
    {
        "label": "OrderedDict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "Counter",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "robotparser",
        "importPath": "urllib",
        "description": "urllib",
        "isExtraImport": true,
        "detail": "urllib",
        "documentation": {}
    },
    {
        "label": "parse",
        "importPath": "urllib",
        "description": "urllib",
        "isExtraImport": true,
        "detail": "urllib",
        "documentation": {}
    },
    {
        "label": "robotparser",
        "importPath": "urllib",
        "description": "urllib",
        "isExtraImport": true,
        "detail": "urllib",
        "documentation": {}
    },
    {
        "label": "requests",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "requests",
        "description": "requests",
        "detail": "requests",
        "documentation": {}
    },
    {
        "label": "xmltodict",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "xmltodict",
        "description": "xmltodict",
        "detail": "xmltodict",
        "documentation": {}
    },
    {
        "label": "Blueprint",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "Blueprint",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "Flask",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "request",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "HtmlTagParser",
        "importPath": "crawler.html_tag_parser",
        "description": "crawler.html_tag_parser",
        "isExtraImport": true,
        "detail": "crawler.html_tag_parser",
        "documentation": {}
    },
    {
        "label": "LinkFinder",
        "importPath": "crawler.link_finder",
        "description": "crawler.link_finder",
        "isExtraImport": true,
        "detail": "crawler.link_finder",
        "documentation": {}
    },
    {
        "label": "Doc",
        "importPath": "models.doc",
        "description": "models.doc",
        "isExtraImport": true,
        "detail": "models.doc",
        "documentation": {}
    },
    {
        "label": "Doc",
        "importPath": "models.doc",
        "description": "models.doc",
        "isExtraImport": true,
        "detail": "models.doc",
        "documentation": {}
    },
    {
        "label": "Doc",
        "importPath": "models.doc",
        "description": "models.doc",
        "isExtraImport": true,
        "detail": "models.doc",
        "documentation": {}
    },
    {
        "label": "re",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "re",
        "description": "re",
        "detail": "re",
        "documentation": {}
    },
    {
        "label": "punctuation",
        "importPath": "string",
        "description": "string",
        "isExtraImport": true,
        "detail": "string",
        "documentation": {}
    },
    {
        "label": "nltk",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "nltk",
        "description": "nltk",
        "detail": "nltk",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk",
        "description": "nltk",
        "isExtraImport": true,
        "detail": "nltk",
        "documentation": {}
    },
    {
        "label": "SnowballStemmer",
        "importPath": "nltk",
        "description": "nltk",
        "isExtraImport": true,
        "detail": "nltk",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk",
        "description": "nltk",
        "isExtraImport": true,
        "detail": "nltk",
        "documentation": {}
    },
    {
        "label": "stopwords",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "wordnet",
        "importPath": "nltk.corpus",
        "description": "nltk.corpus",
        "isExtraImport": true,
        "detail": "nltk.corpus",
        "documentation": {}
    },
    {
        "label": "Keyword",
        "importPath": "models.keyword",
        "description": "models.keyword",
        "isExtraImport": true,
        "detail": "models.keyword",
        "documentation": {}
    },
    {
        "label": "Keyword",
        "importPath": "models.keyword",
        "description": "models.keyword",
        "isExtraImport": true,
        "detail": "models.keyword",
        "documentation": {}
    },
    {
        "label": "db",
        "importPath": "neomodel",
        "description": "neomodel",
        "isExtraImport": true,
        "detail": "neomodel",
        "documentation": {}
    },
    {
        "label": "StringProperty",
        "importPath": "neomodel",
        "description": "neomodel",
        "isExtraImport": true,
        "detail": "neomodel",
        "documentation": {}
    },
    {
        "label": "StructuredNode",
        "importPath": "neomodel",
        "description": "neomodel",
        "isExtraImport": true,
        "detail": "neomodel",
        "documentation": {}
    },
    {
        "label": "BooleanProperty",
        "importPath": "neomodel",
        "description": "neomodel",
        "isExtraImport": true,
        "detail": "neomodel",
        "documentation": {}
    },
    {
        "label": "RelationshipTo",
        "importPath": "neomodel",
        "description": "neomodel",
        "isExtraImport": true,
        "detail": "neomodel",
        "documentation": {}
    },
    {
        "label": "StructuredRel",
        "importPath": "neomodel",
        "description": "neomodel",
        "isExtraImport": true,
        "detail": "neomodel",
        "documentation": {}
    },
    {
        "label": "StructuredNode",
        "importPath": "neomodel",
        "description": "neomodel",
        "isExtraImport": true,
        "detail": "neomodel",
        "documentation": {}
    },
    {
        "label": "StringProperty",
        "importPath": "neomodel",
        "description": "neomodel",
        "isExtraImport": true,
        "detail": "neomodel",
        "documentation": {}
    },
    {
        "label": "RelationshipTo",
        "importPath": "neomodel",
        "description": "neomodel",
        "isExtraImport": true,
        "detail": "neomodel",
        "documentation": {}
    },
    {
        "label": "StructuredRel",
        "importPath": "neomodel",
        "description": "neomodel",
        "isExtraImport": true,
        "detail": "neomodel",
        "documentation": {}
    },
    {
        "label": "IntegerProperty",
        "importPath": "neomodel",
        "description": "neomodel",
        "isExtraImport": true,
        "detail": "neomodel",
        "documentation": {}
    },
    {
        "label": "db",
        "importPath": "neomodel",
        "description": "neomodel",
        "isExtraImport": true,
        "detail": "neomodel",
        "documentation": {}
    },
    {
        "label": "Q",
        "importPath": "neomodel",
        "description": "neomodel",
        "isExtraImport": true,
        "detail": "neomodel",
        "documentation": {}
    },
    {
        "label": "config",
        "importPath": "neomodel",
        "description": "neomodel",
        "isExtraImport": true,
        "detail": "neomodel",
        "documentation": {}
    },
    {
        "label": "LinkFinder",
        "importPath": "link_finder",
        "description": "link_finder",
        "isExtraImport": true,
        "detail": "link_finder",
        "documentation": {}
    },
    {
        "label": "HtmlTagParser",
        "importPath": "html_tag_parser",
        "description": "html_tag_parser",
        "isExtraImport": true,
        "detail": "html_tag_parser",
        "documentation": {}
    },
    {
        "label": "CSVWriter",
        "importPath": "csv_writer",
        "description": "csv_writer",
        "isExtraImport": true,
        "detail": "csv_writer",
        "documentation": {}
    },
    {
        "label": "TxtWrite",
        "importPath": "csv_writer",
        "description": "csv_writer",
        "isExtraImport": true,
        "detail": "csv_writer",
        "documentation": {}
    },
    {
        "label": "TxtRead",
        "importPath": "csv_writer",
        "description": "csv_writer",
        "isExtraImport": true,
        "detail": "csv_writer",
        "documentation": {}
    },
    {
        "label": "csv",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "csv",
        "description": "csv",
        "detail": "csv",
        "documentation": {}
    },
    {
        "label": "BeautifulSoup",
        "importPath": "bs4",
        "description": "bs4",
        "isExtraImport": true,
        "detail": "bs4",
        "documentation": {}
    },
    {
        "label": "HTMLParser",
        "importPath": "html.parser",
        "description": "html.parser",
        "isExtraImport": true,
        "detail": "html.parser",
        "documentation": {}
    },
    {
        "label": "IntegerProperty",
        "importPath": "neomodel.properties",
        "description": "neomodel.properties",
        "isExtraImport": true,
        "detail": "neomodel.properties",
        "documentation": {}
    },
    {
        "label": "ranking",
        "importPath": "process",
        "description": "process",
        "isExtraImport": true,
        "detail": "process",
        "documentation": {}
    },
    {
        "label": "sigmoid",
        "importPath": "process",
        "description": "process",
        "isExtraImport": true,
        "detail": "process",
        "documentation": {}
    },
    {
        "label": "cal_probab",
        "importPath": "process",
        "description": "process",
        "isExtraImport": true,
        "detail": "process",
        "documentation": {}
    },
    {
        "label": "cal_tfidf",
        "importPath": "process",
        "description": "process",
        "isExtraImport": true,
        "detail": "process",
        "documentation": {}
    },
    {
        "label": "CORS",
        "importPath": "flask_cors",
        "description": "flask_cors",
        "isExtraImport": true,
        "detail": "flask_cors",
        "documentation": {}
    },
    {
        "label": "cross_origin",
        "importPath": "flask_cors",
        "description": "flask_cors",
        "isExtraImport": true,
        "detail": "flask_cors",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "time",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "time",
        "description": "time",
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "bp",
        "importPath": "commands.crawl",
        "description": "commands.crawl",
        "isExtraImport": true,
        "detail": "commands.crawl",
        "documentation": {}
    },
    {
        "label": "bp",
        "importPath": "commands.stem",
        "description": "commands.stem",
        "isExtraImport": true,
        "detail": "commands.stem",
        "documentation": {}
    },
    {
        "label": "get_keywords",
        "importPath": "commands.stem",
        "description": "commands.stem",
        "isExtraImport": true,
        "detail": "commands.stem",
        "documentation": {}
    },
    {
        "label": "WordNetLemmatizer",
        "importPath": "nltk.stem",
        "description": "nltk.stem",
        "isExtraImport": true,
        "detail": "nltk.stem",
        "documentation": {}
    },
    {
        "label": "calc_tfidf",
        "importPath": "tfidf",
        "description": "tfidf",
        "isExtraImport": true,
        "detail": "tfidf",
        "documentation": {}
    },
    {
        "label": "math",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "math",
        "description": "math",
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "crawl",
        "kind": 2,
        "importPath": "commands.crawl",
        "description": "commands.crawl",
        "peekOfCode": "def crawl():\n    # add_urls_by_sitemap()\n    # Start scraping and exploring more urls\n    doc = Doc.nodes.first_or_none(title__isnull=True)\n    while doc:\n        html_parser = parse_html(doc)\n        if html_parser:\n            # Done: save data to Url model\n            doc.title = html_parser.title\n            doc.description = html_parser.description",
        "detail": "commands.crawl",
        "documentation": {}
    },
    {
        "label": "parse_html",
        "kind": 2,
        "importPath": "commands.crawl",
        "description": "commands.crawl",
        "peekOfCode": "def parse_html(doc):\n    if doc.title:\n        return\n    print('Crawling: ' + doc.url)\n    try:\n        response = requests.get(doc.url)\n        links = LinkFinder(doc.url)\n        links.feed(response.text)\n    except:\n        return",
        "detail": "commands.crawl",
        "documentation": {}
    },
    {
        "label": "add_urls_by_sitemap",
        "kind": 2,
        "importPath": "commands.crawl",
        "description": "commands.crawl",
        "peekOfCode": "def add_urls_by_sitemap():\n    domains = ['https://stackoverflow.com']\n    for domain in domains:\n        print('Crawling sitemap of ' + domain)\n        rp = get_robot_parser(domain)\n        print('Done parsing robots.txt of ' + domain)\n        if rp.site_maps() is None:\n            continue\n        raw_urls = get_urls_from_sitemap(rp.site_maps()[0])\n        queue_urls = [{'url': url['loc']} for url in raw_urls]",
        "detail": "commands.crawl",
        "documentation": {}
    },
    {
        "label": "get_robot_parser",
        "kind": 2,
        "importPath": "commands.crawl",
        "description": "commands.crawl",
        "peekOfCode": "def get_robot_parser(domain):\n    rp = robotparser.RobotFileParser()\n    rp.set_url(domain + \"/robots.txt\")\n    rp.read()\n    return rp\ndef get_urls_from_sitemap(main_sitemap_url):\n    urls = []\n    sitemap_xml_urls = xmltodict.parse(requests.get(main_sitemap_url).text)[\n        'sitemapindex']['sitemap']\n    for xml_url in sitemap_xml_urls:",
        "detail": "commands.crawl",
        "documentation": {}
    },
    {
        "label": "get_urls_from_sitemap",
        "kind": 2,
        "importPath": "commands.crawl",
        "description": "commands.crawl",
        "peekOfCode": "def get_urls_from_sitemap(main_sitemap_url):\n    urls = []\n    sitemap_xml_urls = xmltodict.parse(requests.get(main_sitemap_url).text)[\n        'sitemapindex']['sitemap']\n    for xml_url in sitemap_xml_urls:\n        raw = xmltodict.parse(requests.get(xml_url['loc']).text)[\n            'urlset']['url']\n        print(raw)\n        exit()\n        urls = urls + (raw if isinstance(raw, list) else [raw])",
        "detail": "commands.crawl",
        "documentation": {}
    },
    {
        "label": "bp",
        "kind": 5,
        "importPath": "commands.crawl",
        "description": "commands.crawl",
        "peekOfCode": "bp = Blueprint('crawler', __name__, cli_group='crawler')\n@bp.cli.command('crawl')\ndef crawl():\n    # add_urls_by_sitemap()\n    # Start scraping and exploring more urls\n    doc = Doc.nodes.first_or_none(title__isnull=True)\n    while doc:\n        html_parser = parse_html(doc)\n        if html_parser:\n            # Done: save data to Url model",
        "detail": "commands.crawl",
        "documentation": {}
    },
    {
        "label": "stem",
        "kind": 2,
        "importPath": "commands.stem",
        "description": "commands.stem",
        "peekOfCode": "def stem():\n    nltk.download(\"stopwords\")\n    nltk.download(\"averaged_perceptron_tagger\")\n    # Start stemming\n    doc = Doc.get_havent_stemmed()\n    while doc:\n        text = doc.title + ' ' + doc.description\n        keys = get_keywords(text=text)\n        # print(keywords.items())\n        # return",
        "detail": "commands.stem",
        "documentation": {}
    },
    {
        "label": "get_keywords",
        "kind": 2,
        "importPath": "commands.stem",
        "description": "commands.stem",
        "peekOfCode": "def get_keywords(text):\n    keywords = []\n    # remove symbols n stuff\n    text = re.sub(r\"[\\W+-_]\", \" \", text)\n    words = word_tokenize(text.lower())\n    for w in words:\n        keyword = stemmer.stem(w)\n        if w in stop_words or keyword in punctuation:\n            continue\n        keywords.append(keyword)",
        "detail": "commands.stem",
        "documentation": {}
    },
    {
        "label": "extract_ne",
        "kind": 2,
        "importPath": "commands.stem",
        "description": "commands.stem",
        "peekOfCode": "def extract_ne(text):\n    words = word_tokenize(text)\n    tags = nltk.pos_tag(words)\n    tree = nltk.ne_chunk(tags, binary=True)\n    ne = []\n    for ent in tree:\n        if hasattr(ent, 'label') and ent.label() == 'NE':\n            print(i for i in ent)\n            ne.append(\" \".join(i[0] for i in ent))\n    return ne",
        "detail": "commands.stem",
        "documentation": {}
    },
    {
        "label": "bp",
        "kind": 5,
        "importPath": "commands.stem",
        "description": "commands.stem",
        "peekOfCode": "bp = Blueprint('stemmer', __name__, cli_group='stemmer')\nstemmer = SnowballStemmer(language=\"english\")\nstop_words = set(stopwords.words(\"english\"))\n@bp.cli.command('stem')\ndef stem():\n    nltk.download(\"stopwords\")\n    nltk.download(\"averaged_perceptron_tagger\")\n    # Start stemming\n    doc = Doc.get_havent_stemmed()\n    while doc:",
        "detail": "commands.stem",
        "documentation": {}
    },
    {
        "label": "stemmer",
        "kind": 5,
        "importPath": "commands.stem",
        "description": "commands.stem",
        "peekOfCode": "stemmer = SnowballStemmer(language=\"english\")\nstop_words = set(stopwords.words(\"english\"))\n@bp.cli.command('stem')\ndef stem():\n    nltk.download(\"stopwords\")\n    nltk.download(\"averaged_perceptron_tagger\")\n    # Start stemming\n    doc = Doc.get_havent_stemmed()\n    while doc:\n        text = doc.title + ' ' + doc.description",
        "detail": "commands.stem",
        "documentation": {}
    },
    {
        "label": "stop_words",
        "kind": 5,
        "importPath": "commands.stem",
        "description": "commands.stem",
        "peekOfCode": "stop_words = set(stopwords.words(\"english\"))\n@bp.cli.command('stem')\ndef stem():\n    nltk.download(\"stopwords\")\n    nltk.download(\"averaged_perceptron_tagger\")\n    # Start stemming\n    doc = Doc.get_havent_stemmed()\n    while doc:\n        text = doc.title + ' ' + doc.description\n        keys = get_keywords(text=text)",
        "detail": "commands.stem",
        "documentation": {}
    },
    {
        "label": "grab_links",
        "kind": 2,
        "importPath": "crawler.crawler",
        "description": "crawler.crawler",
        "peekOfCode": "def grab_links(url, crawled):\n    if url in crawled:\n        return\n    print('Crawling: ' + url)\n    try:\n        response = requests.get(url)\n        html_parser = HtmlTagParser(url, response.text)\n        # Done: append url to crawled.txt\n        TxtWrite('../data/crawled_urls.txt', html_parser.url)\n        # Done: append crawl data to csv",
        "detail": "crawler.crawler",
        "documentation": {}
    },
    {
        "label": "crawl",
        "kind": 2,
        "importPath": "crawler.crawler",
        "description": "crawler.crawler",
        "peekOfCode": "def crawl():\n    crawled = TxtRead('../data/crawled_urls.txt').readStr()\n    # Done: run until out of urls in to_crawl_urls\n    # Done: read from to_crawl_urls and clear file\n    # Done: loop to each url\n    grab_links(BASE_URL, crawled)\n    for x in range(500):\n        nextUrl = TxtRead('../data/to_crawl_urls.txt').readStr()\n        if nextUrl != None:\n            grab_links(nextUrl, crawled)",
        "detail": "crawler.crawler",
        "documentation": {}
    },
    {
        "label": "CSVWriter",
        "kind": 6,
        "importPath": "crawler.csv_writer",
        "description": "crawler.csv_writer",
        "peekOfCode": "class CSVWriter:\n    header = ['url', 'title', 'description']\n    regex = re.compile(\n        r'^(?:http|ftp)s?://' # http:// or https://\n        r'(?:(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\\.)+(?:[A-Z]{2,6}\\.?|[A-Z0-9-]{2,}\\.?)|' #domain...\n        r'localhost|' #localhost...\n        r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3})' # ...or ip\n        r'(?::\\d+)?' # optional port\n        r'(?:/?|[/?]\\S+)$', re.IGNORECASE)\n    def __init__(self, url, title, description):",
        "detail": "crawler.csv_writer",
        "documentation": {}
    },
    {
        "label": "TxtWrite",
        "kind": 6,
        "importPath": "crawler.csv_writer",
        "description": "crawler.csv_writer",
        "peekOfCode": "class TxtWrite:\n    def __init__(self, path, url):\n        super().__init__()\n        self.path = path\n        self.url = url\n        self.write()\n    def write(self):\n        with open(str(self.path), \"a\") as f:\n            if type(self.url) is set:\n                for i in self.url:",
        "detail": "crawler.csv_writer",
        "documentation": {}
    },
    {
        "label": "TxtRead",
        "kind": 6,
        "importPath": "crawler.csv_writer",
        "description": "crawler.csv_writer",
        "peekOfCode": "class TxtRead:\n    def __init__(self, path):\n        super().__init__()\n        self.path = path\n    def readStr(self):\n        url = None\n        data = None\n        with open(str(self.path), \"r\") as f:\n            url = f.readline().rstrip('\\n')\n            data = f.read().splitlines(True)",
        "detail": "crawler.csv_writer",
        "documentation": {}
    },
    {
        "label": "HtmlTagParser",
        "kind": 6,
        "importPath": "crawler.html_tag_parser",
        "description": "crawler.html_tag_parser",
        "peekOfCode": "class HtmlTagParser:\n    def __init__(self, url, html_text, links):\n        self.soup = BeautifulSoup(html_text, 'html.parser')\n        self.url = url\n        self.title = self.get_title()\n        self.description = self.get_description()\n        self.links = links\n    def get_title(self):\n        title = self.soup.find('meta', {'name': 'title'})\n        if title is None:",
        "detail": "crawler.html_tag_parser",
        "documentation": {}
    },
    {
        "label": "LinkFinder",
        "kind": 6,
        "importPath": "crawler.link_finder",
        "description": "crawler.link_finder",
        "peekOfCode": "class LinkFinder(HTMLParser):\n    def __init__(self, page_url):\n        super().__init__()\n        self.page_url = page_url\n        url = parse.urlsplit(page_url)\n        self.base_url = url.scheme + '://' + url.netloc\n        self.robot_parser = get_robot_parser(self.base_url)\n        self.links = set()\n    def handle_starttag(self, tag, attrs):\n        if tag == 'a':",
        "detail": "crawler.link_finder",
        "documentation": {}
    },
    {
        "label": "get_robot_parser",
        "kind": 2,
        "importPath": "crawler.link_finder",
        "description": "crawler.link_finder",
        "peekOfCode": "def get_robot_parser(domain):\n    rp = robotparser.RobotFileParser()\n    rp.set_url(domain + \"/robots.txt\")\n    rp.read()\n    return rp",
        "detail": "crawler.link_finder",
        "documentation": {}
    },
    {
        "label": "RefRelationship",
        "kind": 6,
        "importPath": "models.doc",
        "description": "models.doc",
        "peekOfCode": "class RefRelationship(StructuredRel):\n    pass\nclass Doc(StructuredNode):\n    url = StringProperty(required=True, unique_index=True)\n    title = StringProperty()\n    description = StringProperty()\n    # ref = IntegerProperty(default=0)\n    stemmed = BooleanProperty(default=False)\n    ref_docs = RelationshipTo('Doc', 'REF', model=RefRelationship)\n    @staticmethod",
        "detail": "models.doc",
        "documentation": {}
    },
    {
        "label": "Doc",
        "kind": 6,
        "importPath": "models.doc",
        "description": "models.doc",
        "peekOfCode": "class Doc(StructuredNode):\n    url = StringProperty(required=True, unique_index=True)\n    title = StringProperty()\n    description = StringProperty()\n    # ref = IntegerProperty(default=0)\n    stemmed = BooleanProperty(default=False)\n    ref_docs = RelationshipTo('Doc', 'REF', model=RefRelationship)\n    @staticmethod\n    def get_havent_stemmed():\n        return Doc.nodes.first_or_none(stemmed__exact=False, title__isnull=False, description__isnull=False)",
        "detail": "models.doc",
        "documentation": {}
    },
    {
        "label": "KeywordRelationship",
        "kind": 6,
        "importPath": "models.keyword",
        "description": "models.keyword",
        "peekOfCode": "class KeywordRelationship(StructuredRel):\n    freq = IntegerProperty(required=True)\nclass Keyword(StructuredNode):\n    keyword = StringProperty(unique_index=True)\n    docs = RelationshipTo('Doc', 'IN', model=KeywordRelationship)",
        "detail": "models.keyword",
        "documentation": {}
    },
    {
        "label": "Keyword",
        "kind": 6,
        "importPath": "models.keyword",
        "description": "models.keyword",
        "peekOfCode": "class Keyword(StructuredNode):\n    keyword = StringProperty(unique_index=True)\n    docs = RelationshipTo('Doc', 'IN', model=KeywordRelationship)",
        "detail": "models.keyword",
        "documentation": {}
    },
    {
        "label": "get_time_ms",
        "kind": 2,
        "importPath": "app",
        "description": "app",
        "peekOfCode": "def get_time_ms():\n    return int(round(time_.time() * 1000))\n# function to convert nltk tag to wordnet tag\ndef nltk_tag_to_wordnet_tag(nltk_tag):\n    if nltk_tag.startswith(\"J\"):\n        return wordnet.ADJ\n    elif nltk_tag.startswith(\"V\"):\n        return wordnet.VERB\n    elif nltk_tag.startswith(\"N\"):\n        return wordnet.NOUN",
        "detail": "app",
        "documentation": {}
    },
    {
        "label": "nltk_tag_to_wordnet_tag",
        "kind": 2,
        "importPath": "app",
        "description": "app",
        "peekOfCode": "def nltk_tag_to_wordnet_tag(nltk_tag):\n    if nltk_tag.startswith(\"J\"):\n        return wordnet.ADJ\n    elif nltk_tag.startswith(\"V\"):\n        return wordnet.VERB\n    elif nltk_tag.startswith(\"N\"):\n        return wordnet.NOUN\n    elif nltk_tag.startswith(\"R\"):\n        return wordnet.ADV\n    else:",
        "detail": "app",
        "documentation": {}
    },
    {
        "label": "lemmatize_sentence",
        "kind": 2,
        "importPath": "app",
        "description": "app",
        "peekOfCode": "def lemmatize_sentence(sentence):\n    # tokenize the sentence and find the POS tag for each token\n    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))\n    # tuple of (token, wordnet_tag)\n    wordnet_tagged = map(lambda x: (\n        x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n    lemmatized_sentence = []\n    for word, tag in wordnet_tagged:\n        if tag is None:\n            # if there is no available tag, append the token as is",
        "detail": "app",
        "documentation": {}
    },
    {
        "label": "find_synonyms",
        "kind": 2,
        "importPath": "app",
        "description": "app",
        "peekOfCode": "def find_synonyms(keyword):\n    synonyms = []\n    for synset in wordnet.synsets(keyword):\n        for lemma in synset.lemmas():\n            synonyms.append(lemma.name())\n    return str(synonyms)\n@app.route(\"/\")\ndef hello_world():\n    return \"Hello World!\"\n@app.route(\"/search\")",
        "detail": "app",
        "documentation": {}
    },
    {
        "label": "hello_world",
        "kind": 2,
        "importPath": "app",
        "description": "app",
        "peekOfCode": "def hello_world():\n    return \"Hello World!\"\n@app.route(\"/search\")\n@cross_origin()\ndef search():\n    query = request.args.get(\"query\")\n    start = get_time_ms()\n    # word tokenize\n    # filter stopwords\n    # stemming",
        "detail": "app",
        "documentation": {}
    },
    {
        "label": "search",
        "kind": 2,
        "importPath": "app",
        "description": "app",
        "peekOfCode": "def search():\n    query = request.args.get(\"query\")\n    start = get_time_ms()\n    # word tokenize\n    # filter stopwords\n    # stemming\n    search_keywords = list(get_keywords(query).keys())\n    print(search_keywords)\n    keywords = Keyword.nodes.has(docs=True).filter(keyword__in=search_keywords)\n    response = {}",
        "detail": "app",
        "documentation": {}
    },
    {
        "label": "app",
        "kind": 5,
        "importPath": "app",
        "description": "app",
        "peekOfCode": "app = Flask(__name__)\ncors = CORS(app)\napp.config['CORS_HEADERS'] = 'Content-Type'\napp.register_blueprint(crawl_bp)\napp.register_blueprint(stem_bp)\nNEO_URL = \"bolt://{username}:{password}@{uri}\".format(\n    username=os.getenv(\"DB_USERNAME\"),\n    password=os.getenv(\"DB_PASSWORD\"),\n    uri=os.getenv(\"DB_URI\"),\n)",
        "detail": "app",
        "documentation": {}
    },
    {
        "label": "cors",
        "kind": 5,
        "importPath": "app",
        "description": "app",
        "peekOfCode": "cors = CORS(app)\napp.config['CORS_HEADERS'] = 'Content-Type'\napp.register_blueprint(crawl_bp)\napp.register_blueprint(stem_bp)\nNEO_URL = \"bolt://{username}:{password}@{uri}\".format(\n    username=os.getenv(\"DB_USERNAME\"),\n    password=os.getenv(\"DB_PASSWORD\"),\n    uri=os.getenv(\"DB_URI\"),\n)\ndb.set_connection(NEO_URL)",
        "detail": "app",
        "documentation": {}
    },
    {
        "label": "app.config['CORS_HEADERS']",
        "kind": 5,
        "importPath": "app",
        "description": "app",
        "peekOfCode": "app.config['CORS_HEADERS'] = 'Content-Type'\napp.register_blueprint(crawl_bp)\napp.register_blueprint(stem_bp)\nNEO_URL = \"bolt://{username}:{password}@{uri}\".format(\n    username=os.getenv(\"DB_USERNAME\"),\n    password=os.getenv(\"DB_PASSWORD\"),\n    uri=os.getenv(\"DB_URI\"),\n)\ndb.set_connection(NEO_URL)\nconfig.DATABASE_URL = NEO_URL",
        "detail": "app",
        "documentation": {}
    },
    {
        "label": "NEO_URL",
        "kind": 5,
        "importPath": "app",
        "description": "app",
        "peekOfCode": "NEO_URL = \"bolt://{username}:{password}@{uri}\".format(\n    username=os.getenv(\"DB_USERNAME\"),\n    password=os.getenv(\"DB_PASSWORD\"),\n    uri=os.getenv(\"DB_URI\"),\n)\ndb.set_connection(NEO_URL)\nconfig.DATABASE_URL = NEO_URL\nlemmatizer = WordNetLemmatizer()\ndef get_time_ms():\n    return int(round(time_.time() * 1000))",
        "detail": "app",
        "documentation": {}
    },
    {
        "label": "config.DATABASE_URL",
        "kind": 5,
        "importPath": "app",
        "description": "app",
        "peekOfCode": "config.DATABASE_URL = NEO_URL\nlemmatizer = WordNetLemmatizer()\ndef get_time_ms():\n    return int(round(time_.time() * 1000))\n# function to convert nltk tag to wordnet tag\ndef nltk_tag_to_wordnet_tag(nltk_tag):\n    if nltk_tag.startswith(\"J\"):\n        return wordnet.ADJ\n    elif nltk_tag.startswith(\"V\"):\n        return wordnet.VERB",
        "detail": "app",
        "documentation": {}
    },
    {
        "label": "lemmatizer",
        "kind": 5,
        "importPath": "app",
        "description": "app",
        "peekOfCode": "lemmatizer = WordNetLemmatizer()\ndef get_time_ms():\n    return int(round(time_.time() * 1000))\n# function to convert nltk tag to wordnet tag\ndef nltk_tag_to_wordnet_tag(nltk_tag):\n    if nltk_tag.startswith(\"J\"):\n        return wordnet.ADJ\n    elif nltk_tag.startswith(\"V\"):\n        return wordnet.VERB\n    elif nltk_tag.startswith(\"N\"):",
        "detail": "app",
        "documentation": {}
    },
    {
        "label": "sigmoid",
        "kind": 2,
        "importPath": "process",
        "description": "process",
        "peekOfCode": "def sigmoid(num):\n    return math.exp(num) / (math.exp(num) + 1)\ndef cal_probab(query, docs):\n    N = len(docs)\n    for doc in docs:\n        score = 0\n        for word in query:\n            try:\n                ni = doc['freqs'][word]\n                if ni > 0:",
        "detail": "process",
        "documentation": {}
    },
    {
        "label": "cal_probab",
        "kind": 2,
        "importPath": "process",
        "description": "process",
        "peekOfCode": "def cal_probab(query, docs):\n    N = len(docs)\n    for doc in docs:\n        score = 0\n        for word in query:\n            try:\n                ni = doc['freqs'][word]\n                if ni > 0:\n                    score += math.log2((N+0.5) / ni)\n            except KeyError:",
        "detail": "process",
        "documentation": {}
    },
    {
        "label": "ranking",
        "kind": 2,
        "importPath": "process",
        "description": "process",
        "peekOfCode": "def ranking(docs):\n    docs = docs.copy()\n    for doc in docs:\n        doc['score'] = sigmoid((doc['scores']['tf_idf'] * 0.5 + doc['scores']['probab'] * 0.25 + doc['scores']['ref'] * 0.25) / 3)\n    return sorted(\n        docs, key=lambda doc: doc[\"score\"], reverse=True)\ndef tf(ni):\n    return 0 if ni <= 0 else 1 + math.log2(ni)\ndef idf(N, n_docs_appear_in):\n    return 0 if n_docs_appear_in <= 0 else math.log2(N / n_docs_appear_in)",
        "detail": "process",
        "documentation": {}
    },
    {
        "label": "tf",
        "kind": 2,
        "importPath": "process",
        "description": "process",
        "peekOfCode": "def tf(ni):\n    return 0 if ni <= 0 else 1 + math.log2(ni)\ndef idf(N, n_docs_appear_in):\n    return 0 if n_docs_appear_in <= 0 else math.log2(N / n_docs_appear_in)\ndef cal_tfidf(search_keywords, docs):\n    N = len(docs)\n    for doc in docs:\n        score = 0\n        for search_keyword in search_keywords:\n            if not search_keyword in doc['freqs']:",
        "detail": "process",
        "documentation": {}
    },
    {
        "label": "idf",
        "kind": 2,
        "importPath": "process",
        "description": "process",
        "peekOfCode": "def idf(N, n_docs_appear_in):\n    return 0 if n_docs_appear_in <= 0 else math.log2(N / n_docs_appear_in)\ndef cal_tfidf(search_keywords, docs):\n    N = len(docs)\n    for doc in docs:\n        score = 0\n        for search_keyword in search_keywords:\n            if not search_keyword in doc['freqs']:\n                continue\n            score += tf(doc['freqs'][search_keyword]) * idf(N, n_docs_appear_in=sum([1 if search_keyword in doc['freqs'] else 0 for doc in docs]))",
        "detail": "process",
        "documentation": {}
    },
    {
        "label": "cal_tfidf",
        "kind": 2,
        "importPath": "process",
        "description": "process",
        "peekOfCode": "def cal_tfidf(search_keywords, docs):\n    N = len(docs)\n    for doc in docs:\n        score = 0\n        for search_keyword in search_keywords:\n            if not search_keyword in doc['freqs']:\n                continue\n            score += tf(doc['freqs'][search_keyword]) * idf(N, n_docs_appear_in=sum([1 if search_keyword in doc['freqs'] else 0 for doc in docs]))\n        doc['scores']['tf_idf'] = sigmoid(score)",
        "detail": "process",
        "documentation": {}
    },
    {
        "label": "extract_ne",
        "kind": 2,
        "importPath": "test",
        "description": "test",
        "peekOfCode": "def extract_ne(text):\n    words = word_tokenize(text)\n    tags = nltk.pos_tag(words)\n    tree = nltk.ne_chunk(tags, binary=True)\n    ne = set()\n    for ent in tree:\n        if hasattr(ent, 'label') and ent.label() == 'NE':\n            ne.add(\" \".join(i[0] for i in ent))\n    return ne\ndoc = extract_ne('European authorities fined Google a record $5.1 billion on Wednesday for abusing its power in the mobile phone market and ordered the company to alter its practices.'",
        "detail": "test",
        "documentation": {}
    },
    {
        "label": "doc",
        "kind": 5,
        "importPath": "test",
        "description": "test",
        "peekOfCode": "doc = extract_ne('European authorities fined Google a record $5.1 billion on Wednesday for abusing its power in the mobile phone market and ordered the company to alter its practices.'\n                 ' Information Retrieval has.')\nprint(doc)",
        "detail": "test",
        "documentation": {}
    },
    {
        "label": "calc_score",
        "kind": 2,
        "importPath": "tfidf",
        "description": "tfidf",
        "peekOfCode": "def calc_score(q, d, all_d):\n    score = 0\n    for t in q:\n        score += tf(t, d) * idf(t, all_d)\n    return score\ndef tf(t, d):\n    return float(d.count(t)) / len(d)\ndef idf(t, all_d):\n    count_in_doc = 0\n    for d in all_d:",
        "detail": "tfidf",
        "documentation": {}
    },
    {
        "label": "tf",
        "kind": 2,
        "importPath": "tfidf",
        "description": "tfidf",
        "peekOfCode": "def tf(t, d):\n    return float(d.count(t)) / len(d)\ndef idf(t, all_d):\n    count_in_doc = 0\n    for d in all_d:\n        if t in d[\"content\"]:\n            count_in_doc += 1\n    return 0 if count_in_doc == 0 else math.log(len(all_d) / float(count_in_doc), 2)\ndef calc_tfidf(query, docs):\n    results = []",
        "detail": "tfidf",
        "documentation": {}
    },
    {
        "label": "idf",
        "kind": 2,
        "importPath": "tfidf",
        "description": "tfidf",
        "peekOfCode": "def idf(t, all_d):\n    count_in_doc = 0\n    for d in all_d:\n        if t in d[\"content\"]:\n            count_in_doc += 1\n    return 0 if count_in_doc == 0 else math.log(len(all_d) / float(count_in_doc), 2)\ndef calc_tfidf(query, docs):\n    results = []\n    for doc in docs: \n        score = calc_score(query, doc[\"content\"], docs)",
        "detail": "tfidf",
        "documentation": {}
    },
    {
        "label": "calc_tfidf",
        "kind": 2,
        "importPath": "tfidf",
        "description": "tfidf",
        "peekOfCode": "def calc_tfidf(query, docs):\n    results = []\n    for doc in docs: \n        score = calc_score(query, doc[\"content\"], docs)\n        if score > 0:\n            results.append({**doc, \"score\": score})\n    for res in results:\n        del res[\"content\"]\n    return results",
        "detail": "tfidf",
        "documentation": {}
    }
]